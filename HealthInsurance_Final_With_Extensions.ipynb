{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c9df6f",
   "metadata": {},
   "source": [
    "# Health Insurance Premium Prediction using Machine Learning\n",
    "This notebook implements and extends the methodology described in the paper:  \n",
    "**\"Machine Learning-Based Regression Framework to Predict Health Insurance Premiums\"**  \n",
    "DOI: [10.3390/ijerph19137898](https://doi.org/10.3390/ijerph19137898)\n",
    "\n",
    "We implement the baseline models and introduce three key extensions:\n",
    "1. Hyperparameter Tuning\n",
    "2. Advanced Regression Models (CatBoost, Ensemble)\n",
    "3. SHAP-based Explainability\n",
    "\n",
    "Metrics: **MAE**, **MSE**, **RMSE**, **R²**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "url = \"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Encode categorical features\n",
    "df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Split features and target\n",
    "X = df_encoded.drop(\"charges\", axis=1)\n",
    "y = df_encoded[\"charges\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"XGBoost\": xgb.XGBRegressor(objective='reg:squarederror')\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    results[name] = {\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"MSE\": mean_squared_error(y_test, y_pred),\n",
    "        \"RMSE\": mean_squared_error(y_test, y_pred, squared=False),\n",
    "        \"R2\": r2_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "pd.DataFrame(results).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbdb589",
   "metadata": {},
   "source": [
    "## Extension 1: Hyperparameter Tuning with RandomizedSearchCV\n",
    "We apply tuning on Random Forest and XGBoost using cross-validation to improve prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39eaa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Random Forest tuning\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "rf = RandomForestRegressor()\n",
    "rf_random = RandomizedSearchCV(rf, rf_params, cv=3, n_iter=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "rf_random.fit(X_train_scaled, y_train)\n",
    "rf_best = rf_random.best_estimator_\n",
    "\n",
    "# XGBoost tuning\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "xgb_random = RandomizedSearchCV(xgb_model, xgb_params, cv=3, n_iter=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "xgb_random.fit(X_train_scaled, y_train)\n",
    "xgb_best = xgb_random.best_estimator_\n",
    "\n",
    "# Evaluate tuned models\n",
    "for name, model in [(\"Tuned RF\", rf_best), (\"Tuned XGBoost\", xgb_best)]:\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"{name} -> RMSE: {mean_squared_error(y_test, y_pred, squared=False):.2f}, R²: {r2_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac7aa26",
   "metadata": {},
   "source": [
    "## Extension 2: Advanced Models (CatBoost + Ensemble)\n",
    "We integrate CatBoost and build an ensemble model using VotingRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# Train CatBoost\n",
    "catboost = CatBoostRegressor(verbose=0)\n",
    "catboost.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Ensemble Voting Regressor\n",
    "ensemble = VotingRegressor([(\"catboost\", catboost), (\"xgb\", xgb_best), (\"rf\", rf_best)])\n",
    "ensemble.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate ensemble\n",
    "y_pred_ensemble = ensemble.predict(X_test_scaled)\n",
    "print(f\"Ensemble -> RMSE: {mean_squared_error(y_test, y_pred_ensemble, squared=False):.2f}, R²: {r2_score(y_test, y_pred_ensemble):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd1378",
   "metadata": {},
   "source": [
    "## Extension 3: Explainability with SHAP\n",
    "We use SHAP to explain the predictions of our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.Explainer(ensemble.predict, X_test_scaled)\n",
    "shap_values = explainer(X_test_scaled[:100])\n",
    "shap.plots.beeswarm(shap_values)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
